{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 수정한 점\n",
        "\n",
        "1. hidden layer 3으로 조정\n",
        "2. Hidden unit : 256\n",
        "3. lr=0.001 / 학습 중 Learning Rate Scheduler를 사용해 점진적으로 감소시킴\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oEss4zUd2el2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. STL10 데이터셋 클래스 정의\n",
        "STL10 데이터셋을 학습 및 테스트 데이터로 로드하고, 파이토치 데이터로더를 통해 배치 단위로 데이터를 준비하는 작업을 수행   \n",
        "해당 데이터셋을 모댈 학습에 사용할 수 있도록 전처리와 로더 생성까지 포함"
      ],
      "metadata": {
        "id": "ReVerRMbzYq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# 데이터 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    # 데이터 증강\n",
        "    transforms.RandomHorizontalFlip(),  # 랜덤 가로 뒤집기\n",
        "    transforms.RandomRotation(10),     # 랜덤 회전\n",
        "\n",
        "\n",
        "    transforms.ToTensor(),       # 이미지를 텐서로 변환\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 정규화\n",
        "])\n",
        "\n",
        "# STL10 학습 및 테스트 데이터셋 다운로드\n",
        "train_dataset = datasets.STL10(root='./data', split='train', transform=transform, download=True)\n",
        "test_dataset = datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "# Training 데이터를 Validation Set으로 나누기\n",
        "train_indices, val_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42) # Training 데이터의 일부를 Validation Set으로 나누어 모델 성능을 지속적으로 평가\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "# DataLoader 생성 (64개의 배치로 나누고, 학습 데이터를 셔플)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-fC9n7ygWGA",
        "outputId": "539f62b2-3637-49e5-b0d4-1df7db539e06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.64G/2.64G [04:54<00:00, 8.95MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/stl10_binary.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# MLP 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers, output_size, activation_function):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        previous_size = input_size\n",
        "\n",
        "        for _ in range(n_layers):\n",
        "            layers.append(nn.Linear(previous_size, hidden_size))\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))  # 배치 정규화\n",
        "            layers.append(activation_function())\n",
        "            layers.append(nn.Dropout(0.3))  # 드롭아웃 확률 0.3으로 감소함\n",
        "            previous_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(previous_size, output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# STL10 이미지 크기: 96x96, 채널 3 (3채널 이미지를 펼침)\n",
        "input_size = 96 * 96 * 3\n",
        "hidden_size = 256 # hidden unit의 개수를 128에서 점점 줄여서 나가도 됨 -- 만약 줄인다면--> 특징 추출을 세밀화\n",
        "n_layers = 3  # Hidden layer 개수\n",
        "output_size = 10  # 클래스 개수\n",
        "\n",
        "\n",
        "activation_function = nn.LeakyReLU # 활성화 함수 변경함\n",
        "\n",
        "model = MLP(input_size, hidden_size, n_layers, output_size, activation_function)"
      ],
      "metadata": {
        "id": "ofci_uYcgXZW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer 설계\n",
        "lr = 0.001\n",
        "\n",
        "# 에포크 설정\n",
        "epochs = 100\n",
        "\n",
        "# 옵티마이저와 스케줄러\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.5) # # 매 20 에포크마다 학습률을 반으로 줄임\n",
        "\n",
        "\n",
        "# epoch 별 train loss / validation loss / accuracy loss 저장할 list --> 매번 출력하여 추이 파악\n",
        "list_epoch = []\n",
        "list_train_loss = []\n",
        "list_val_loss = []\n",
        "list_acc = []\n",
        "list_acc_epoch = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "# 학습 함수 정의\n",
        "def train_model(model, dataloader, val_loader, criterion, optimizer, epochs, patience=4):\n",
        "    # train_losses = []  # Store losses for each epoch\n",
        "    # val_losses = []  # Store validation losses for each epoch\n",
        "    # accuracies = []  # Store accuracies for each epoch\n",
        "    best_val_loss = float('inf')  # 초기 최상의 val_loss를 무한대로 설정\n",
        "    patience_counter = 0  # Early Stopping 카운터 초기화\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, labels in dataloader:\n",
        "\n",
        "            images = images.view(images.size(0), -1)  # 이미지를 1D 벡터로 변환\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)  # 모델에 입력\n",
        "\n",
        "            loss = criterion(outputs, labels)  # 손실 계산\n",
        "            loss.backward()  # 역전파\n",
        "            optimizer.step()  # 파라미터 업데이트\n",
        "            train_loss = train_loss + loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(dataloader)\n",
        "        list_train_loss.append(train_loss)\n",
        "        list_epoch.append(epoch)\n",
        "\n",
        "        # validation loss\n",
        "        val_loss = evaluate_loss(model, val_loader, criterion)\n",
        "        list_val_loss.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Early Stopping 조건 확인 - val_loss가 증가하면 더이상 학습을 하지 않는 로직\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0  # 개선이 있으면 카운터 리셋\n",
        "        else:\n",
        "            patience_counter += 1  # 개선되지 않으면 카운터 증가\n",
        "            print(f\"Early Stopping Counter: {patience_counter}/{patience}\")\n",
        "\n",
        "            if patience_counter >= patience:  # patience에 도달하면 학습 종료\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # 스케줄러 업데이트\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# loss function 평가\n",
        "def evaluate_loss(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.view(images.size(0), -1)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "# 평가 함수 정의\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # 평가 시에는 그래디언트 계산 비활성화\n",
        "        for images, labels in dataloader:\n",
        "            images = images.view(images.size(0), -1)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # 예측 값 추출\n",
        "            correct = correct + (predicted == labels).sum().item()\n",
        "            total = total + labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    list_acc.append(accuracy)\n",
        "    list_acc_epoch.append(len(list_epoch))\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "UNZO0Tl_gYd2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-t0NeXvgarP",
        "outputId": "a8f27cb8-7857-4df7-f3da-bf9e7440ee17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 1.7892, Val Loss: 1.7173\n",
            "Epoch [2/100], Train Loss: 1.7106, Val Loss: 1.6819\n",
            "Epoch [3/100], Train Loss: 1.6651, Val Loss: 1.6801\n",
            "Epoch [4/100], Train Loss: 1.6203, Val Loss: 1.6594\n",
            "Epoch [5/100], Train Loss: 1.5772, Val Loss: 1.6559\n",
            "Epoch [6/100], Train Loss: 1.5592, Val Loss: 1.6596\n",
            "Epoch [7/100], Train Loss: 1.5455, Val Loss: 1.6197\n",
            "Epoch [8/100], Train Loss: 1.4927, Val Loss: 1.5988\n",
            "Epoch [9/100], Train Loss: 1.4681, Val Loss: 1.5952\n",
            "Epoch [10/100], Train Loss: 1.4493, Val Loss: 1.5816\n",
            "Epoch [11/100], Train Loss: 1.4305, Val Loss: 1.5874\n",
            "Epoch [12/100], Train Loss: 1.4231, Val Loss: 1.5679\n",
            "Epoch [13/100], Train Loss: 1.3971, Val Loss: 1.5685\n",
            "Epoch [14/100], Train Loss: 1.3801, Val Loss: 1.5886\n",
            "Epoch [15/100], Train Loss: 1.3524, Val Loss: 1.6334\n",
            "Epoch [16/100], Train Loss: 1.3498, Val Loss: 1.5626\n",
            "Epoch [17/100], Train Loss: 1.3425, Val Loss: 1.5637\n",
            "Epoch [18/100], Train Loss: 1.3315, Val Loss: 1.5625\n",
            "Epoch [19/100], Train Loss: 1.3095, Val Loss: 1.5368\n",
            "Epoch [20/100], Train Loss: 1.2740, Val Loss: 1.5484\n",
            "Epoch [21/100], Train Loss: 1.2258, Val Loss: 1.5111\n",
            "Epoch [22/100], Train Loss: 1.2178, Val Loss: 1.5246\n",
            "Epoch [23/100], Train Loss: 1.1740, Val Loss: 1.5283\n",
            "Epoch [24/100], Train Loss: 1.1430, Val Loss: 1.5130\n",
            "Epoch [25/100], Train Loss: 1.1741, Val Loss: 1.5423\n",
            "Epoch [26/100], Train Loss: 1.1481, Val Loss: 1.5571\n",
            "Epoch [27/100], Train Loss: 1.1403, Val Loss: 1.5497\n",
            "Epoch [28/100], Train Loss: 1.1215, Val Loss: 1.5167\n",
            "Epoch [29/100], Train Loss: 1.1048, Val Loss: 1.5470\n",
            "Epoch [30/100], Train Loss: 1.0986, Val Loss: 1.5566\n",
            "Epoch [31/100], Train Loss: 1.0748, Val Loss: 1.5649\n",
            "Epoch [32/100], Train Loss: 1.0897, Val Loss: 1.5671\n",
            "Epoch [33/100], Train Loss: 1.0827, Val Loss: 1.5359\n",
            "Epoch [34/100], Train Loss: 1.0634, Val Loss: 1.5649\n",
            "Epoch [35/100], Train Loss: 1.0595, Val Loss: 1.5544\n",
            "Epoch [36/100], Train Loss: 1.0550, Val Loss: 1.5488\n",
            "Epoch [37/100], Train Loss: 1.0162, Val Loss: 1.5627\n",
            "Epoch [38/100], Train Loss: 1.0318, Val Loss: 1.5496\n",
            "Epoch [39/100], Train Loss: 1.0102, Val Loss: 1.5613\n",
            "Epoch [40/100], Train Loss: 1.0146, Val Loss: 1.5172\n",
            "Epoch [41/100], Train Loss: 0.9620, Val Loss: 1.5493\n",
            "Epoch [42/100], Train Loss: 0.9529, Val Loss: 1.5786\n",
            "Epoch [43/100], Train Loss: 0.9278, Val Loss: 1.5909\n",
            "Epoch [44/100], Train Loss: 0.9121, Val Loss: 1.5595\n",
            "Epoch [45/100], Train Loss: 0.9215, Val Loss: 1.5751\n",
            "Epoch [46/100], Train Loss: 0.9029, Val Loss: 1.5931\n",
            "Epoch [47/100], Train Loss: 0.9142, Val Loss: 1.5945\n",
            "Epoch [48/100], Train Loss: 0.9132, Val Loss: 1.5539\n",
            "Epoch [49/100], Train Loss: 0.8919, Val Loss: 1.5958\n",
            "Epoch [50/100], Train Loss: 0.8889, Val Loss: 1.5792\n",
            "Epoch [51/100], Train Loss: 0.8999, Val Loss: 1.5872\n",
            "Epoch [52/100], Train Loss: 0.8630, Val Loss: 1.5957\n",
            "Epoch [53/100], Train Loss: 0.8735, Val Loss: 1.6046\n",
            "Epoch [54/100], Train Loss: 0.8710, Val Loss: 1.6268\n",
            "Epoch [55/100], Train Loss: 0.8595, Val Loss: 1.6107\n",
            "Epoch [56/100], Train Loss: 0.8544, Val Loss: 1.6103\n",
            "Epoch [57/100], Train Loss: 0.8450, Val Loss: 1.6089\n",
            "Epoch [58/100], Train Loss: 0.8332, Val Loss: 1.6512\n",
            "Epoch [59/100], Train Loss: 0.8277, Val Loss: 1.6133\n",
            "Epoch [60/100], Train Loss: 0.8457, Val Loss: 1.6501\n",
            "Epoch [61/100], Train Loss: 0.8452, Val Loss: 1.6444\n",
            "Epoch [62/100], Train Loss: 0.8050, Val Loss: 1.6216\n",
            "Epoch [63/100], Train Loss: 0.8050, Val Loss: 1.6310\n",
            "Epoch [64/100], Train Loss: 0.7918, Val Loss: 1.6319\n",
            "Epoch [65/100], Train Loss: 0.8089, Val Loss: 1.6336\n",
            "Epoch [66/100], Train Loss: 0.8002, Val Loss: 1.6355\n",
            "Epoch [67/100], Train Loss: 0.8069, Val Loss: 1.6428\n",
            "Epoch [68/100], Train Loss: 0.7749, Val Loss: 1.6712\n",
            "Epoch [69/100], Train Loss: 0.7903, Val Loss: 1.6276\n",
            "Epoch [70/100], Train Loss: 0.7565, Val Loss: 1.6494\n",
            "Epoch [71/100], Train Loss: 0.7752, Val Loss: 1.6543\n",
            "Epoch [72/100], Train Loss: 0.7669, Val Loss: 1.6635\n",
            "Epoch [73/100], Train Loss: 0.7896, Val Loss: 1.6464\n",
            "Epoch [74/100], Train Loss: 0.7588, Val Loss: 1.6282\n",
            "Epoch [75/100], Train Loss: 0.7373, Val Loss: 1.6715\n",
            "Epoch [76/100], Train Loss: 0.7701, Val Loss: 1.6597\n",
            "Epoch [77/100], Train Loss: 0.7661, Val Loss: 1.6637\n",
            "Epoch [78/100], Train Loss: 0.7435, Val Loss: 1.6800\n",
            "Epoch [79/100], Train Loss: 0.7318, Val Loss: 1.7045\n",
            "Epoch [80/100], Train Loss: 0.7407, Val Loss: 1.6797\n",
            "Epoch [81/100], Train Loss: 0.7250, Val Loss: 1.6867\n",
            "Epoch [82/100], Train Loss: 0.7207, Val Loss: 1.7075\n",
            "Epoch [83/100], Train Loss: 0.7414, Val Loss: 1.6735\n",
            "Epoch [84/100], Train Loss: 0.7293, Val Loss: 1.6222\n",
            "Epoch [85/100], Train Loss: 0.7163, Val Loss: 1.6764\n",
            "Epoch [86/100], Train Loss: 0.7220, Val Loss: 1.6848\n",
            "Epoch [87/100], Train Loss: 0.7447, Val Loss: 1.6721\n",
            "Epoch [88/100], Train Loss: 0.7356, Val Loss: 1.6664\n",
            "Epoch [89/100], Train Loss: 0.7086, Val Loss: 1.6675\n",
            "Epoch [90/100], Train Loss: 0.7147, Val Loss: 1.6714\n",
            "Epoch [91/100], Train Loss: 0.7352, Val Loss: 1.6740\n",
            "Epoch [92/100], Train Loss: 0.7110, Val Loss: 1.6892\n",
            "Epoch [93/100], Train Loss: 0.6957, Val Loss: 1.6910\n",
            "Epoch [94/100], Train Loss: 0.7100, Val Loss: 1.6760\n",
            "Epoch [95/100], Train Loss: 0.6853, Val Loss: 1.7078\n",
            "Epoch [96/100], Train Loss: 0.7025, Val Loss: 1.6840\n",
            "Epoch [97/100], Train Loss: 0.7022, Val Loss: 1.7023\n",
            "Epoch [98/100], Train Loss: 0.7206, Val Loss: 1.6722\n",
            "Epoch [99/100], Train Loss: 0.7012, Val Loss: 1.6817\n",
            "Epoch [100/100], Train Loss: 0.7084, Val Loss: 1.6786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "accuracy = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsxy7iyBzxpZ",
        "outputId": "ef1196ce-478d-4afa-ec76-78f340319766"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 48.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결과물 기록\n",
        "hidden_size = 256 / hidden layer = 3 / lr = 0.001 / 100 epoch / Act functino : LeakyReLU / Dropout : 0.3 / scheduler = StepLR(optimizer, step_size=20, gamma=0.5)- Accuracy: 48.89%"
      ],
      "metadata": {
        "id": "wTKcX8cO8nXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 피드백\n",
        "\n",
        "train loss는 에포크 늘어나면 무조건 줄으니까 train loss가 줄어든다고 해서 좋은 게 아니고 val loss를 잘 보자. 그리고 early stopping 추가하여 이상한 길로 가는 것 같으면 학습을 중단하자.\n",
        "\n",
        "# 우성님 지원님 피드백\n",
        "파라미터가 너무 적음\n",
        "27,648(96x96x3)에서 256(hiddenUnit 설정 값)로 추출하고 있는데, 이는 시작부터 고된 추출일수도 있음을 인지하기. 그래서 모델의 파라미터 규모를 키우는 방법도 생각해보세요.\n"
      ],
      "metadata": {
        "id": "Msqg18bDHI6e"
      }
    }
  ]
}