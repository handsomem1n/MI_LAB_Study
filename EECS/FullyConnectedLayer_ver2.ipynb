{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 수정한 점\n",
        "\n",
        "1. 데이터의 다양화를 위하여 이미지를 가로로 뒤집거나 회전시켜서 데이터 증강함\n",
        "2. 노드 dropout 적용\n",
        "3. 이전의 방법에서 validation_loss가 점점 증가하던 것을 막고자 training data의 일부(0.1~0.2)를 validation data로 사용하였음\n",
        "4. (몇번 학습해보니 loss가 감소하는 추세를 보이길래)hidden_unit의 수를 늘렸음 / hidden_layer의 수를 4개로 늘렸음\n",
        "\n"
      ],
      "metadata": {
        "id": "oEss4zUd2el2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. STL10 데이터셋 클래스 정의\n",
        "STL10 데이터셋을 학습 및 테스트 데이터로 로드하고, 파이토치 데이터로더를 통해 배치 단위로 데이터를 준비하는 작업을 수행   \n",
        "해당 데이터셋을 모댈 학습에 사용할 수 있도록 전처리와 로더 생성까지 포함"
      ],
      "metadata": {
        "id": "ReVerRMbzYq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# 데이터 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    # 데이터 증강\n",
        "    transforms.RandomHorizontalFlip(),  # 랜덤 가로 뒤집기\n",
        "    transforms.RandomRotation(10),     # 랜덤 회전\n",
        "\n",
        "\n",
        "    transforms.ToTensor(),       # 이미지를 텐서로 변환\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 정규화\n",
        "])\n",
        "\n",
        "# STL10 학습 및 테스트 데이터셋 다운로드\n",
        "train_dataset = datasets.STL10(root='./data', split='train', transform=transform, download=True)\n",
        "test_dataset = datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "# Training 데이터를 Validation Set으로 나누기\n",
        "train_indices, val_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "# DataLoader 생성 (64개의 배치로 나누고, 학습 데이터를 셔플)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-fC9n7ygWGA",
        "outputId": "eee311ee-83c7-40d3-d75c-052ee0a4219c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# MLP 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, n_layers, output_size, activation_function):\n",
        "        \"\"\"\n",
        "        Flexible한 MLP Model\n",
        "        Args:\n",
        "            input_size: Input 차원\n",
        "            hidden_sizes : List of hidden layer sizes.\n",
        "            output_size: Output 차원\n",
        "            activation_fn : \b여러개 중 선택해보기\n",
        "        \"\"\"\n",
        "\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "\n",
        "        layers = []\n",
        "        previous_size = input_size\n",
        "\n",
        "        # Hidden layers 생성\n",
        "        for _ in range(n_layers):\n",
        "            layers.append(nn.Linear(previous_size, hidden_size))  # Linear layer\n",
        "            layers.append(activation_function())  # Activation function\n",
        "\n",
        "            layers.append(nn.Dropout(0.5))  # 드롭아웃\n",
        "\n",
        "            previous_size = hidden_size\n",
        "\n",
        "        # Output layer 생성\n",
        "        layers.append(nn.Linear(previous_size, output_size))\n",
        "        self.model = nn.Sequential(*layers) # layers라는 list에 넣었던 입력층의 결과들을 다시 Sequential 모델로 결합\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# STL10 이미지 크기: 96x96, 채널 3 (3채널 이미지를 펼침)\n",
        "input_size = 96 * 96 * 3\n",
        "hidden_size = 256 # hidden unit의 개수를 128에서 점점 줄여서 나가도 됨 -- 만약 줄인다면--> 특징 추출을 세밀화\n",
        "n_layers = 4  # Hidden layer 개수\n",
        "output_size = 10  # 클래스 개수\n",
        "\n",
        "\n",
        "activation_function = nn.ReLU # 활성화 함수 설정\n",
        "\n",
        "model = MLP(input_size, hidden_size, n_layers, output_size, activation_function)"
      ],
      "metadata": {
        "id": "ofci_uYcgXZW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer 설계\n",
        "lr = 0.0005\n",
        "\n",
        "# 에포크 설정\n",
        "epochs = 100\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "\n",
        "# epoch 별 train loss / validation loss / accuracy loss 저장할 list --> 매번 출력하여 추이 파악\n",
        "list_epoch = []\n",
        "list_train_loss = []\n",
        "list_val_loss = []\n",
        "list_acc = []\n",
        "list_acc_epoch = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "# 학습 함수 정의\n",
        "def train_model(model, dataloader, val_loader, criterion, optimizer, epochs):\n",
        "    train_losses = []  # Store losses for each epoch\n",
        "    val_losses = []  # Store validation losses for each epoch\n",
        "    accuracies = []  # Store accuracies for each epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, labels in dataloader:\n",
        "\n",
        "            images = images.view(images.size(0), -1)  # 이미지를 1D 벡터로 변환\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)  # 모델에 입력\n",
        "\n",
        "            loss = criterion(outputs, labels)  # 손실 계산\n",
        "            loss.backward()  # 역전파\n",
        "            optimizer.step()  # 파라미터 업데이트\n",
        "            train_loss = train_loss + loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(dataloader)\n",
        "        list_train_loss.append(train_loss)\n",
        "        list_epoch.append(epoch)\n",
        "\n",
        "        # validation loss\n",
        "        val_loss = evaluate_loss(model, val_loader, criterion)\n",
        "        list_val_loss.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# loss function 평가\n",
        "def evaluate_loss(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.view(images.size(0), -1)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "# 평가 함수 정의\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # 평가 시에는 그래디언트 계산 비활성화\n",
        "        for images, labels in dataloader:\n",
        "            images = images.view(images.size(0), -1)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # 예측 값 추출\n",
        "            correct = correct + (predicted == labels).sum().item()\n",
        "            total = total + labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    list_acc.append(accuracy)\n",
        "    list_acc_epoch.append(len(list_epoch))\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "UNZO0Tl_gYd2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-t0NeXvgarP",
        "outputId": "d369980e-c964-4ed6-bbc9-d56ab34f5dc1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Train Loss: 2.3025, Val Loss: 2.3042\n",
            "Epoch [2/2], Train Loss: 2.3022, Val Loss: 2.3042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "accuracy = evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "Fsxy7iyBzxpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결과물 기록\n",
        "hidden_size = 256 / hidden layer = 4 / lr = 0.0005 / 100 epoch - Accuracy: 43.01%"
      ],
      "metadata": {
        "id": "wTKcX8cO8nXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# train_loss / val_loss / accuracy를 그래프로 찍어보자\n"
      ],
      "metadata": {
        "id": "549XUQqwyWDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_results(epochs, train_losses, val_losses, accuracies):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Train Loss와 Val Loss 그래프\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
        "    plt.plot(epochs, val_losses, label='Validation Loss', color='orange')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy 그래프 (accuracies가 비어있지 않을 경우)\n",
        "    if accuracies:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, accuracies, label='Accuracy', color='green')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Training Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iGY927dyyVgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAqL7RQPvgtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}