{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEss4zUd2el2"
      },
      "source": [
        "# 수정한 점\n",
        "\n",
        "1. hidden layer 3으로 조정\n",
        "2. Hidden unit : 2048 -> 512 -> 256\n",
        "3. lr=0.0005\n",
        "4. Color Jitter , Random Crop 추가하여 데이터 증강 (우성님 피드백)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReVerRMbzYq2"
      },
      "source": [
        "## 1. STL10 데이터셋 클래스 정의\n",
        "STL10 데이터셋을 학습 및 테스트 데이터로 로드하고, 파이토치 데이터로더를 통해 배치 단위로 데이터를 준비하는 작업을 수행   \n",
        "해당 데이터셋을 모댈 학습에 사용할 수 있도록 전처리와 로더 생성까지 포함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-fC9n7ygWGA",
        "outputId": "46f0d022-4c36-43c9-9c26-860e472eff4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.64G/2.64G [11:35<00:00, 3.80MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/stl10_binary.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# 데이터 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # 50% 확률로 가로 뒤집기\n",
        "    transforms.RandomRotation(10),     # ±10도 회전\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 밝기, 대비, 채도, 색조 +- 20%로 조정\n",
        "    transforms.RandomCrop(80, padding=8),  # 이미지를 80x80으로 무작위 자르기 (padding 추가)\n",
        "    transforms.Resize((96, 96)),  # 원래 크기인 96x96으로 복원\n",
        "    transforms.ToTensor(),       # 이미지를 텐서로 변환\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 정규화\n",
        "])\n",
        "\n",
        "# STL10 학습 및 테스트 데이터셋 다운로드\n",
        "train_dataset = datasets.STL10(root='./data', split='train', transform=transform, download=True)\n",
        "test_dataset = datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "# Training 데이터를 Validation Set으로 나누기\n",
        "train_indices, val_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "# DataLoader 생성 (64개의 배치로 나누고, 학습 데이터를 셔플)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofci_uYcgXZW",
        "outputId": "656174fc-becf-456f-ad94-d3718ffbdbe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=27648, out_features=2048, bias=True)\n",
            "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.3, inplace=False)\n",
            "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.3, inplace=False)\n",
            "    (12): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "# MLP 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_function):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        previous_size = input_size\n",
        "\n",
        "        # Hidden layers 생성\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(previous_size, hidden_size))  # Linear layer\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))  # 배치 정규화\n",
        "            layers.append(activation_function())  # Activation function\n",
        "\n",
        "            layers.append(nn.Dropout(0.3))  # 드롭아웃\n",
        "\n",
        "            previous_size = hidden_size\n",
        "\n",
        "        # Output layer 생성\n",
        "        layers.append(nn.Linear(previous_size, output_size))  # 최종 출력층\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# 모델 파라미터\n",
        "input_size = 96 * 96 * 3  # 입력 크기\n",
        "hidden_sizes = [2048, 512, 256]  # 점진적으로 크기 줄이도록 수정 / hidden layer의 개수는  hiddensize.length와 같음\n",
        "output_size = 10  # 클래스 개수\n",
        "activation_function = nn.ReLU  # 활성화 함수\n",
        "\n",
        "# 모델 생성\n",
        "model = MLP(input_size, hidden_sizes, output_size, activation_function)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNZO0Tl_gYd2"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer 설계\n",
        "lr = 0.001\n",
        "\n",
        "# 에포크 설정\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "# 옵티마이저와 스케줄러\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "\n",
        "# epoch 별 train loss / validation loss / accuracy loss 저장할 list --> 매번 출력하여 추이 파악\n",
        "list_epoch = []\n",
        "list_train_loss = []\n",
        "list_val_loss = []\n",
        "list_acc = []\n",
        "list_acc_epoch = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "# 학습 함수 정의\n",
        "def train_model(model, dataloader, val_loader, criterion, optimizer, epochs, patience=5):\n",
        "    # train_losses = []  # Store losses for each epoch\n",
        "    # val_losses = []  # Store validation losses for each epoch\n",
        "    # accuracies = []  # Store accuracies for each epoch\n",
        "\n",
        "    best_val_loss = float('inf')  # 초기 최상의 val_loss를 무한대로 설정\n",
        "    patience_counter = 0  # Early Stopping 카운터 초기화\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, labels in dataloader:\n",
        "\n",
        "            images = images.view(images.size(0), -1)  # 이미지를 1D 벡터로 변환\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)  # 모델에 입력\n",
        "\n",
        "            loss = criterion(outputs, labels)  # 손실 계산\n",
        "            loss.backward()  # 역전파\n",
        "            optimizer.step()  # 파라미터 업데이트\n",
        "            train_loss = train_loss + loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(dataloader)\n",
        "        list_train_loss.append(train_loss)\n",
        "        list_epoch.append(epoch)\n",
        "\n",
        "        # validation loss\n",
        "        val_loss = evaluate_loss(model, val_loader, criterion)\n",
        "        list_val_loss.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Early Stopping 조건 확인 - val_loss가 증가하면 더이상 학습을 하지 않는 로직\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0  # 개선이 있으면 카운터 리셋\n",
        "        else:\n",
        "            patience_counter += 1  # 개선되지 않으면 카운터 증가\n",
        "            print(f\"Early Stopping Counter: {patience_counter}/{patience}\")\n",
        "\n",
        "            if patience_counter >= patience:  # patience에 도달하면 학습 종료\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # 스케줄러 업데이트\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# loss function 평가\n",
        "def evaluate_loss(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.view(images.size(0), -1)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "# 평가 함수 정의\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # 평가 시에는 그래디언트 계산 비활성화\n",
        "        for images, labels in dataloader:\n",
        "            images = images.view(images.size(0), -1)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # 예측 값 추출\n",
        "            correct = correct + (predicted == labels).sum().item()\n",
        "            total = total + labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    list_acc.append(accuracy)\n",
        "    list_acc_epoch.append(len(list_epoch))\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-t0NeXvgarP",
        "outputId": "bc0633e6-78f7-45e6-b075-aee36886eb67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Train Loss: 1.7031, Val Loss: 1.6879\n",
            "Epoch [2/100], Train Loss: 1.6960, Val Loss: 1.7091\n",
            "Early Stopping Counter: 1/5\n",
            "Epoch [3/100], Train Loss: 1.6621, Val Loss: 1.6884\n",
            "Early Stopping Counter: 2/5\n",
            "Epoch [4/100], Train Loss: 1.6739, Val Loss: 1.6965\n",
            "Early Stopping Counter: 3/5\n",
            "Epoch [5/100], Train Loss: 1.6645, Val Loss: 1.6824\n",
            "Epoch [6/100], Train Loss: 1.6394, Val Loss: 1.6882\n",
            "Early Stopping Counter: 1/5\n",
            "Epoch [7/100], Train Loss: 1.6470, Val Loss: 1.7134\n",
            "Early Stopping Counter: 2/5\n",
            "Epoch [8/100], Train Loss: 1.6452, Val Loss: 1.6973\n",
            "Early Stopping Counter: 3/5\n",
            "Epoch [9/100], Train Loss: 1.6541, Val Loss: 1.6759\n",
            "Epoch [10/100], Train Loss: 1.6322, Val Loss: 1.6738\n",
            "Epoch [11/100], Train Loss: 1.6447, Val Loss: 1.6737\n",
            "Epoch [12/100], Train Loss: 1.6298, Val Loss: 1.6622\n",
            "Epoch [13/100], Train Loss: 1.6200, Val Loss: 1.6703\n",
            "Early Stopping Counter: 1/5\n",
            "Epoch [14/100], Train Loss: 1.6279, Val Loss: 1.6718\n",
            "Early Stopping Counter: 2/5\n",
            "Epoch [15/100], Train Loss: 1.6049, Val Loss: 1.6717\n",
            "Early Stopping Counter: 3/5\n",
            "Epoch [16/100], Train Loss: 1.6223, Val Loss: 1.6540\n",
            "Epoch [17/100], Train Loss: 1.6054, Val Loss: 1.6659\n",
            "Early Stopping Counter: 1/5\n",
            "Epoch [18/100], Train Loss: 1.6115, Val Loss: 1.6516\n",
            "Epoch [19/100], Train Loss: 1.6259, Val Loss: 1.6580\n",
            "Early Stopping Counter: 1/5\n",
            "Epoch [20/100], Train Loss: 1.6061, Val Loss: 1.6574\n",
            "Early Stopping Counter: 2/5\n",
            "Epoch [21/100], Train Loss: 1.5834, Val Loss: 1.6788\n",
            "Early Stopping Counter: 3/5\n",
            "Epoch [22/100], Train Loss: 1.5982, Val Loss: 1.6448\n",
            "Epoch [23/100], Train Loss: 1.5963, Val Loss: 1.6366\n",
            "Epoch [24/100], Train Loss: 1.5456, Val Loss: 1.6121\n",
            "Epoch [25/100], Train Loss: 1.5458, Val Loss: 1.6281\n",
            "Early Stopping Counter: 1/5\n",
            "Epoch [26/100], Train Loss: 1.5658, Val Loss: 1.6372\n",
            "Early Stopping Counter: 2/5\n",
            "Epoch [27/100], Train Loss: 1.5465, Val Loss: 1.6459\n",
            "Early Stopping Counter: 3/5\n",
            "Epoch [28/100], Train Loss: 1.5603, Val Loss: 1.6350\n",
            "Early Stopping Counter: 4/5\n",
            "Epoch [29/100], Train Loss: 1.5650, Val Loss: 1.6248\n",
            "Early Stopping Counter: 5/5\n",
            "Early stopping triggered at epoch 29\n"
          ]
        }
      ],
      "source": [
        "# 모델 학습\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsxy7iyBzxpZ",
        "outputId": "604952af-8ee2-4b19-aa27-d78a0476b409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 41.76%\n"
          ]
        }
      ],
      "source": [
        "# 모델 평가\n",
        "accuracy = evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTKcX8cO8nXw"
      },
      "source": [
        "# 결과물 기록\n",
        "hidden_sizes = [2048, 512, 256] / hidden layer = 3 / lr = 0.001 / 100 epoch / Act functino : \\ReLU / Dropout : 0.3 / scheduler = StepLR(optimizer, step_size=20, gamma=0.5)- Accuracy: 48.89%"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}