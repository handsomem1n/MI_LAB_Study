{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 작업 순서\n",
        "1. STL10 데이터셋 클래스 정의\n",
        "- 이전에 CustomImageDataset 클래스를 구현한 것처럼, STL10 데이터를 위한 데이터셋 클래스 생성\n",
        "- 데이터셋 클래스는 STL10의 이미지와 레이블을 불러오고, 필요한 전처리를 수행\n",
        "2. MLP(Multi-Layer Perceptron) 모델 구성\n",
        "- STL10 데이터셋을 학습하기 위해 다층 퍼셉트론(MLP) 모델로 구현\n",
        "- 입력층 → 은닉층(여러 층) → 출력층으로 구성\n",
        "3. 모델 학습\n",
        "- 데이터셋을 사용해 MLP 모델을 학습\n",
        "- 손실 함수와 옵티마이저를 설정, 에폭별 학습을 수행\n",
        "4. 테스트 및 Accuracy 측정\n",
        "- 테스트 데이터셋으로 모델의 성능(Accuracy)을 평가"
      ],
      "metadata": {
        "id": "z6DZgQr5zQmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. STL10 데이터셋 클래스 정의\n",
        "STL10 데이터셋을 학습 및 테스트 데이터로 로드하고, 파이토치 데이터로더를 통해 배치 단위로 데이터를 준비하는 작업을 수행   \n",
        "해당 데이터셋을 모댈 학습에 사용할 수 있도록 전처리와 로더 생성까지 포함"
      ],
      "metadata": {
        "id": "ReVerRMbzYq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 데이터 변환 정의\n",
        "# 데이터 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((96, 96)),  # STL10 크기에 맞\u001f\n",
        "    transforms.ToTensor(),       # 이미지를 텐서로 변환\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 정규화\n",
        "])\n",
        "\n",
        "# STL10 학습 및 테스트 데이터셋 다운로드\n",
        "train_dataset = datasets.STL10(root=\"data\", split=\"train\", download=True, transform=transform)\n",
        "test_dataset = datasets.STL10(root=\"data\", split=\"test\", download=True, transform=transform)\n",
        "val_dataset = datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "# DataLoader 생성 (64개의 배치로 나누고, 학습 데이터를 셔플)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-fC9n7ygWGA",
        "outputId": "3214ba19-3690-4c16-d49f-33e657d460cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터 변환 정의\n",
        "transforms.Compose:  \n",
        "- 여러 변환을 순서대로 적용하기 위해 사용  \n",
        "transforms.ToTensor():  \n",
        "- 이미지를 파이토치 텐서로 변환    \n",
        "- 픽셀 값을 [0,255] 에서 [0,1] 범위로 정규화\n",
        "#### STL10 데이터셋 로드\n",
        "STL10 데이터셋:   \n",
        "- 이미지 크기: 96x96 RGB(3채널)   \n",
        "- 클래스: 10개 클래스   \n",
        "- 데이터 구성:  \n",
        "    - 학습 데이터: 5000개\n",
        "    - 테스트 데이터: 8000개\n",
        "매개변수:\n",
        "- root=\"data\":\n",
        "    - 데이터셋이 저장될 디렉토리 경로\n",
        "    - 디렉토리에 데이터가 없으면,download= True 로 다운\n",
        "- split=\"train\"/split=\"test\":\n",
        "    - split 매개변수를 사용해 학습 데이터와 테스트 데이터를 나눔\n",
        "- transform=transform:\n",
        "    - 데이터에 정의한 변환(ToTensor)을 적용\n",
        "\n",
        "#### DataLoader 생성\n",
        "DataLoader:\n",
        "- PyTorch에서 제공하는 데이터로더로 데이터를 배치 단위로 나눠 모델 학습에 적합한 형태로 제공\n",
        "매개변수:\n",
        "- train_dataset/ test_dataset:\n",
        "    - STL10의 학습 및 테스트 데이터셋 객체\n",
        "- batch_size = 64:\n",
        "    - 한 번에 반환할 데이터 샘플의 개수(64개)\n",
        "    - 학습 데이터 5,000개 → 5,000 ÷ 64 ≈ 78개의 배치\n",
        "- shuffle = True:\n",
        "    - 학습 데이터는 에폭마다 랜덤하게 섞어 학습의 다양성 보장\n",
        "    - 모델이 데이터 순서에 의존하지 않도록 방지\n",
        "- shuffle = False:  \n",
        "    - 테스트 데이터는 순서를 유지"
      ],
      "metadata": {
        "id": "jtRRAPS70gMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ozvH1pWDynqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 어떤 관점으로 푸려고 했느냐\n",
        "## hyperparameter를 구하기 위해 한 것들\n",
        "\n",
        "> 1. 학습과 모델의 성능에 영향을 주는 Activation Function / learning rate / Loss function 값들을 각각 지정해봄\n",
        "2. 학습시키고, evaluation을 하면서 Loss를 찍어봄.\n",
        "3. loss가 낮아지는 추세가 계속 될 때까지 학습\n",
        "3. loss가 낮아지다가 다시 올라가는 구간에서 다른 parameter설정과의 비교 및 원인 분석\n",
        "4. 다른 parameter설정과의 비교"
      ],
      "metadata": {
        "id": "3yqCCOPOQwoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# MLP 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, n_layers, output_size, activation_function):\n",
        "        \"\"\"\n",
        "        Flexible한 MLP Model\n",
        "        Args:\n",
        "            input_size: Input 차원\n",
        "            hidden_sizes : List of hidden layer sizes.\n",
        "            output_size: Output 차원\n",
        "            activation_fn : \b여러개 중 선택해보기\n",
        "        \"\"\"\n",
        "\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "\n",
        "        layers = []\n",
        "        previous_size = input_size\n",
        "\n",
        "        # Hidden layers 생성\n",
        "        for _ in range(n_layers):\n",
        "            layers.append(nn.Linear(previous_size, hidden_size))  # Linear layer\n",
        "            layers.append(activation_function())  # Activation function\n",
        "            previous_size = hidden_size\n",
        "\n",
        "        # Output layer 생성\n",
        "        layers.append(nn.Linear(previous_size, output_size))\n",
        "\n",
        "        # layers라는 list에 넣었던 입력층의 결과들을 다시 Sequential 모델로 결합\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# STL10 이미지 크기: 96x96, 채널 3 (3채널 이미지를 펼침)\n",
        "input_size = 96 * 96 * 3\n",
        "hidden_size = 128 # hidden unit의 개수를 128에서 점점 줄여서 나가도 됨 --> 특징 추출을 세밀화\n",
        "n_layers = 3  # Hidden layer 개수\n",
        "output_size = 10  # 클래스 개수\n",
        "\n",
        "\n",
        "activation_function = nn.ReLU # 활성화 함수 설정\n",
        "\n",
        "model = MLP(input_size, hidden_size, n_layers, output_size, activation_function)"
      ],
      "metadata": {
        "id": "ofci_uYcgXZW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer 설계\n",
        "lr = 0.001\n",
        "\n",
        "# 에포크 설정\n",
        "epochs = 100\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "\n",
        "# epoch 별 train loss / validation loss / accuracy loss 저장할 list --> 매번 출력하여 추이 파악\n",
        "list_epoch = []\n",
        "list_train_loss = []\n",
        "list_val_loss = []\n",
        "list_acc = []\n",
        "list_acc_epoch = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "# 학습 함수 정의\n",
        "def train_model(model, dataloader, val_loader, criterion, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, labels in dataloader:\n",
        "\n",
        "            images = images.view(images.size(0), -1)  # 이미지를 1D 벡터로 변환\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)  # 모델에 입력\n",
        "\n",
        "            loss = criterion(outputs, labels)  # 손실 계산\n",
        "            loss.backward()  # 역전파\n",
        "            optimizer.step()  # 파라미터 업데이트\n",
        "            train_loss = train_loss + loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(dataloader)\n",
        "        list_train_loss.append(train_loss)\n",
        "        list_epoch.append(epoch)\n",
        "\n",
        "        # validation loss\n",
        "        val_loss = evaluate_loss(model, val_loader, criterion)\n",
        "        list_val_loss.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# loss function 평가\n",
        "def evaluate_loss(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.view(images.size(0), -1)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "# 평가 함수 정의\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # 평가 시에는 그래디언트 계산 비활성화\n",
        "        for images, labels in dataloader:\n",
        "            images = images.view(images.size(0), -1)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # 예측 값 추출\n",
        "            correct = correct + (predicted == labels).sum().item()\n",
        "            total = total + labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    list_acc.append(accuracy)\n",
        "    list_acc_epoch.append(len(list_epoch))\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# print('Epoch: {}, Train Loss: {}, Val Loss: {}, Test Acc: {}%'.format(i, train_loss, val_loss, acc*100))"
      ],
      "metadata": {
        "id": "UNZO0Tl_gYd2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
        "\n",
        "# 모델 평가\n",
        "accuracy = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-t0NeXvgarP",
        "outputId": "d4bb2813-c436-4b08-c872-4305363353e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 1.8931, Val Loss: 1.8248\n",
            "Epoch [2/100], Train Loss: 1.6378, Val Loss: 1.7176\n",
            "Epoch [3/100], Train Loss: 1.4348, Val Loss: 1.7531\n",
            "Epoch [4/100], Train Loss: 1.2661, Val Loss: 1.7350\n",
            "Epoch [5/100], Train Loss: 1.1256, Val Loss: 1.8816\n",
            "Epoch [6/100], Train Loss: 0.9557, Val Loss: 1.9219\n",
            "Epoch [7/100], Train Loss: 0.7563, Val Loss: 2.1898\n",
            "Epoch [8/100], Train Loss: 0.6202, Val Loss: 2.3719\n",
            "Epoch [9/100], Train Loss: 0.5576, Val Loss: 2.5618\n",
            "Epoch [10/100], Train Loss: 0.4750, Val Loss: 2.7176\n",
            "Epoch [11/100], Train Loss: 0.3792, Val Loss: 3.0919\n",
            "Epoch [12/100], Train Loss: 0.4975, Val Loss: 2.8794\n",
            "Epoch [13/100], Train Loss: 0.2524, Val Loss: 3.1523\n",
            "Epoch [14/100], Train Loss: 0.2660, Val Loss: 3.4166\n",
            "Epoch [15/100], Train Loss: 0.1761, Val Loss: 3.7746\n",
            "Epoch [16/100], Train Loss: 0.1919, Val Loss: 3.6923\n",
            "Epoch [17/100], Train Loss: 0.1933, Val Loss: 3.7389\n",
            "Epoch [18/100], Train Loss: 0.1670, Val Loss: 3.9706\n",
            "Epoch [19/100], Train Loss: 0.1678, Val Loss: 3.9592\n",
            "Epoch [20/100], Train Loss: 0.2354, Val Loss: 3.9820\n",
            "Epoch [21/100], Train Loss: 0.1404, Val Loss: 4.3426\n",
            "Epoch [22/100], Train Loss: 0.1279, Val Loss: 4.2655\n",
            "Epoch [23/100], Train Loss: 0.1578, Val Loss: 4.4573\n",
            "Epoch [24/100], Train Loss: 0.1262, Val Loss: 4.2281\n",
            "Epoch [25/100], Train Loss: 0.1007, Val Loss: 4.5066\n",
            "Epoch [26/100], Train Loss: 0.0442, Val Loss: 4.7528\n",
            "Epoch [27/100], Train Loss: 0.0543, Val Loss: 5.0234\n",
            "Epoch [28/100], Train Loss: 0.0811, Val Loss: 4.9427\n",
            "Epoch [29/100], Train Loss: 0.1222, Val Loss: 4.9975\n",
            "Epoch [30/100], Train Loss: 0.1770, Val Loss: 4.8737\n",
            "Epoch [31/100], Train Loss: 0.1172, Val Loss: 4.9478\n",
            "Epoch [32/100], Train Loss: 0.1177, Val Loss: 4.9318\n",
            "Epoch [33/100], Train Loss: 0.0765, Val Loss: 5.1423\n",
            "Epoch [34/100], Train Loss: 0.0973, Val Loss: 5.5568\n",
            "Epoch [35/100], Train Loss: 0.2310, Val Loss: 4.9825\n",
            "Epoch [36/100], Train Loss: 0.0980, Val Loss: 4.8286\n",
            "Epoch [37/100], Train Loss: 0.0949, Val Loss: 5.0961\n",
            "Epoch [38/100], Train Loss: 0.1373, Val Loss: 4.9758\n",
            "Epoch [39/100], Train Loss: 0.0584, Val Loss: 5.3048\n",
            "Epoch [40/100], Train Loss: 0.0251, Val Loss: 5.4458\n",
            "Epoch [41/100], Train Loss: 0.0175, Val Loss: 5.6298\n",
            "Epoch [42/100], Train Loss: 0.0113, Val Loss: 5.7706\n",
            "Epoch [43/100], Train Loss: 0.0424, Val Loss: 6.0720\n",
            "Epoch [44/100], Train Loss: 0.1415, Val Loss: 5.7185\n",
            "Epoch [45/100], Train Loss: 0.2642, Val Loss: 4.8556\n",
            "Epoch [46/100], Train Loss: 0.1208, Val Loss: 5.1968\n",
            "Epoch [47/100], Train Loss: 0.0655, Val Loss: 5.1977\n",
            "Epoch [48/100], Train Loss: 0.0510, Val Loss: 5.4330\n",
            "Epoch [49/100], Train Loss: 0.0743, Val Loss: 5.5178\n",
            "Epoch [50/100], Train Loss: 0.0391, Val Loss: 5.5699\n",
            "Epoch [51/100], Train Loss: 0.0734, Val Loss: 5.6895\n",
            "Epoch [52/100], Train Loss: 0.0579, Val Loss: 5.9789\n",
            "Epoch [53/100], Train Loss: 0.0744, Val Loss: 5.9563\n",
            "Epoch [54/100], Train Loss: 0.0822, Val Loss: 5.9030\n",
            "Epoch [55/100], Train Loss: 0.1618, Val Loss: 5.5127\n",
            "Epoch [56/100], Train Loss: 0.0507, Val Loss: 5.8219\n",
            "Epoch [57/100], Train Loss: 0.0247, Val Loss: 5.9520\n",
            "Epoch [58/100], Train Loss: 0.0167, Val Loss: 6.3034\n",
            "Epoch [59/100], Train Loss: 0.2756, Val Loss: 5.4172\n",
            "Epoch [60/100], Train Loss: 0.1707, Val Loss: 5.3197\n",
            "Epoch [61/100], Train Loss: 0.0639, Val Loss: 5.3455\n",
            "Epoch [62/100], Train Loss: 0.0264, Val Loss: 5.7008\n",
            "Epoch [63/100], Train Loss: 0.0173, Val Loss: 5.9883\n",
            "Epoch [64/100], Train Loss: 0.0122, Val Loss: 6.1035\n",
            "Epoch [65/100], Train Loss: 0.0070, Val Loss: 6.2476\n",
            "Epoch [66/100], Train Loss: 0.0064, Val Loss: 6.1467\n",
            "Epoch [67/100], Train Loss: 0.0036, Val Loss: 6.2448\n",
            "Epoch [68/100], Train Loss: 0.0007, Val Loss: 6.2770\n",
            "Epoch [69/100], Train Loss: 0.0003, Val Loss: 6.3286\n",
            "Epoch [70/100], Train Loss: 0.0003, Val Loss: 6.3651\n",
            "Epoch [71/100], Train Loss: 0.0002, Val Loss: 6.4011\n",
            "Epoch [72/100], Train Loss: 0.0002, Val Loss: 6.4325\n",
            "Epoch [73/100], Train Loss: 0.0002, Val Loss: 6.4654\n",
            "Epoch [74/100], Train Loss: 0.0002, Val Loss: 6.4957\n",
            "Epoch [75/100], Train Loss: 0.0001, Val Loss: 6.5245\n",
            "Epoch [76/100], Train Loss: 0.0001, Val Loss: 6.5526\n",
            "Epoch [77/100], Train Loss: 0.0001, Val Loss: 6.5796\n",
            "Epoch [78/100], Train Loss: 0.0001, Val Loss: 6.6041\n",
            "Epoch [79/100], Train Loss: 0.0001, Val Loss: 6.6296\n",
            "Epoch [80/100], Train Loss: 0.0001, Val Loss: 6.6539\n",
            "Epoch [81/100], Train Loss: 0.0001, Val Loss: 6.6775\n",
            "Epoch [82/100], Train Loss: 0.0001, Val Loss: 6.7007\n",
            "Epoch [83/100], Train Loss: 0.0001, Val Loss: 6.7240\n",
            "Epoch [84/100], Train Loss: 0.0001, Val Loss: 6.7463\n",
            "Epoch [85/100], Train Loss: 0.0001, Val Loss: 6.7681\n",
            "Epoch [86/100], Train Loss: 0.0001, Val Loss: 6.7903\n",
            "Epoch [87/100], Train Loss: 0.0001, Val Loss: 6.8118\n",
            "Epoch [88/100], Train Loss: 0.0001, Val Loss: 6.8339\n",
            "Epoch [89/100], Train Loss: 0.0001, Val Loss: 6.8549\n",
            "Epoch [90/100], Train Loss: 0.0001, Val Loss: 6.8744\n",
            "Epoch [91/100], Train Loss: 0.0000, Val Loss: 6.8955\n",
            "Epoch [92/100], Train Loss: 0.0000, Val Loss: 6.9151\n",
            "Epoch [93/100], Train Loss: 0.0000, Val Loss: 6.9361\n",
            "Epoch [94/100], Train Loss: 0.0000, Val Loss: 6.9561\n",
            "Epoch [95/100], Train Loss: 0.0000, Val Loss: 6.9770\n",
            "Epoch [96/100], Train Loss: 0.0000, Val Loss: 6.9972\n",
            "Epoch [97/100], Train Loss: 0.0000, Val Loss: 7.0176\n",
            "Epoch [98/100], Train Loss: 0.0000, Val Loss: 7.0380\n",
            "Epoch [99/100], Train Loss: 0.0000, Val Loss: 7.0587\n",
            "Epoch [100/100], Train Loss: 0.0000, Val Loss: 7.0773\n",
            "Accuracy: 41.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 느낀점\n",
        "Train Loss가 점점 줄어들었지만, val loss는 늘어났음.\n",
        "STL에서 validation 데이터를 제공하지 않아서, 이를 확인하기 위해서 각 epoch마다 train loss, validation loss를 찍어보았음.\n",
        "epoch가 반복함에 따라 모델이 학습되지 않은 데이터에서는 오버피팅이 일어나고 있다는 것을 알 수 있었으나, accuracy는 올라가는 발견할 수 있었음.\n",
        "\n",
        "이 코드에서는 아무래도 validation 과정이 없어서 모델이 학습과정에서 잘 하고 있는지를 잘 파악하지 못 하고 있더라고요. (epoch 마다 validation loss를 출력했더니 학습마다 loss가 증가함)\n",
        "\n",
        "그래서 제가 추가적으로 해볼 거는 train 데이터 중 일부를 validation데이터로 분리(데이터 비중은 최소 0.1~ 최대 0.2)하고 각각의 학습마다 validation 거쳐서 오버피팅 막아보고자 했다.(비중에 따른 cross validation을 적용하여 각각의 값을 비교하고 우수한 것을 고르는 비교하여 최적의 k값을 고를 수 있겠는데 이 과정은 한다해도 다음 step에서 진행하는 게 맞는 것 같음)\n",
        "\n",
        "우선 위 과정만 해도 val_loss에서 오버피팅은 줄어들 것 같았음\n"
      ],
      "metadata": {
        "id": "Jqi-aOde0lUT"
      }
    }
  ]
}