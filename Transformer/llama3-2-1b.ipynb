{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a104c50c-39fa-46c2-b6d5-e8a8f7cc4a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:03:41.424034Z",
     "iopub.status.busy": "2025-02-25T01:03:41.423255Z",
     "iopub.status.idle": "2025-02-25T01:03:43.600590Z",
     "shell.execute_reply": "2025-02-25T01:03:43.598707Z",
     "shell.execute_reply.started": "2025-02-25T01:03:41.423967Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets peft bitsandbytes accelerate matplotlib\n",
    "\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f4ea54-0d09-4d97-a29e-b96bb681fd17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:04:21.497621Z",
     "iopub.status.busy": "2025-02-25T01:04:21.496884Z",
     "iopub.status.idle": "2025-02-25T01:04:40.745167Z",
     "shell.execute_reply": "2025-02-25T01:04:40.743334Z",
     "shell.execute_reply.started": "2025-02-25T01:04:21.497557Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551240f5a09d4c2a9070e4b82c6eaa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba86dc49fe6c47789a81eeb58c3c2428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    eos_token = '<|end_of_text|>'  # Llama 모델의 종료 토큰\n",
    "    korQuAD_prompt = \"\"\"\n",
    "    # 학습시킬 Prompt의 형태\n",
    "        ### Question:\n",
    "        {}\n",
    "\n",
    "        ### Context:\n",
    "        {}\n",
    "\n",
    "        ### Answer:\n",
    "        {}\n",
    "    \"\"\"\n",
    "    instructions = examples[\"question\"]\n",
    "    inputs = examples[\"context\"]\n",
    "    outputs = [item['text'][0] for item in examples[\"answers\"]]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = korQuAD_prompt.format(instruction, input, output) + eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# KorQuAD 데이터셋 로딩 (train과 validation 세트가 있다고 가정)\n",
    "raw_datasets = load_dataset(\"KorQuAD/squad_kor_v1\")\n",
    "\n",
    "# train, validation 세트에 포맷팅 함수 적용\n",
    "formatted_train = raw_datasets[\"train\"].map(formatting_prompts_func, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "formatted_valid = raw_datasets[\"validation\"].map(formatting_prompts_func, batched=True, remove_columns=raw_datasets[\"validation\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39eb50a3-77ea-4073-b11f-8f015b4a457c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:07:57.684498Z",
     "iopub.status.busy": "2025-02-25T01:07:57.683784Z",
     "iopub.status.idle": "2025-02-25T01:08:38.493771Z",
     "shell.execute_reply": "2025-02-25T01:08:38.492369Z",
     "shell.execute_reply.started": "2025-02-25T01:07:57.684437Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35e14effb384a56b6dcca9408c1fff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344e230590cf4edf9475db3b69454622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저 로딩 및 데이터셋 토큰화\n",
    "model_name = \"meta-llama/Llama-3.2-1B-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_length = 512  # 최대 토큰 길이 설정 # 1024하면 안됨\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 예제 텍스트를 토큰화 (truncation, padding 적용)\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "# train, validation 데이터셋 토큰화\n",
    "tokenized_train = formatted_train.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_valid = formatted_valid.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa3d038-4066-493b-abff-6b68a55bc3eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:09:20.232633Z",
     "iopub.status.busy": "2025-02-25T01:09:20.231853Z",
     "iopub.status.idle": "2025-02-25T01:10:49.314041Z",
     "shell.execute_reply": "2025-02-25T01:10:49.312155Z",
     "shell.execute_reply.started": "2025-02-25T01:09:20.232560Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "config = BitsAndBytesConfig(load_in_4bit = True) # 4bit quantization\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=config, # 8 or 4 bit Quan인데 4bit로 성능보다는 학습 시간의 단축에 focus\n",
    "    device_map=\"auto\" # quantization과 lora는 gpu 상에서만 가능한 거로 알고 있어서 gpu할당이 끊어지면 학습도 중단될 것 같긴한데 그냥 auto로 둘래\n",
    ")\n",
    "\n",
    "# QLoRA를 위한 준비 (모델의 특정 부분을 학습 가능하게 변경)\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d896ae56-1544-44ea-9db1-892d7e9be7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:13:28.998634Z",
     "iopub.status.busy": "2025-02-25T01:13:28.997623Z",
     "iopub.status.idle": "2025-02-25T01:13:29.010069Z",
     "shell.execute_reply": "2025-02-25T01:13:29.008315Z",
     "shell.execute_reply.started": "2025-02-25T01:13:28.998573Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024022144\n"
     ]
    }
   ],
   "source": [
    "check_memory = model.get_memory_footprint()\n",
    "print(\"vram 사용량 : \", check_memory / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f5ddce-34dd-4dfb-9015-b9a5ae6b4696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:18:20.708220Z",
     "iopub.status.busy": "2025-02-25T01:18:20.707309Z",
     "iopub.status.idle": "2025-02-25T01:18:20.716653Z",
     "shell.execute_reply": "2025-02-25T01:18:20.714706Z",
     "shell.execute_reply.started": "2025-02-25T01:18:20.708160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lora hyperparam\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",   # 언어 모델 작업\n",
    "    inference_mode=False,\n",
    "    r=8,                    # 기본값\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "    \"q_proj\", # 입력 벡터를 query로 변환하여 self-attention에서 사용\n",
    "    # \"o_proj\", \n",
    "    # \"k_proj\", \n",
    "    \"v_proj\", # 입력 벡터를 value로 변환함, attention의 가중합 계산에 사용\n",
    "    # \"gate_proj\", \n",
    "    # \"up_proj\", \n",
    "    # \"down_proj\"\n",
    "    ],\n",
    "    bias= 'none'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aed34f64-2ad8-4102-af47-171f88e63826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:38:49.948276Z",
     "iopub.status.busy": "2025-02-25T01:38:49.947576Z",
     "iopub.status.idle": "2025-02-25T01:38:50.026525Z",
     "shell.execute_reply": "2025-02-25T01:38:50.025323Z",
     "shell.execute_reply.started": "2025-02-25T01:38:49.948217Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027430016\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.use_cache = False # 양자화 기법 적용할 경우 캐시 관련 설정때문에 loss 계산이 누락되는 경우가 있다고 함\n",
    "\n",
    "after_lora_config = model.get_memory_footprint()\n",
    "print(\"after lora vram usage : \", after_lora_config / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7477562b-f3db-4698-ad32-e9117b56215c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:38:51.988947Z",
     "iopub.status.busy": "2025-02-25T01:38:51.988259Z",
     "iopub.status.idle": "2025-02-25T01:38:52.061299Z",
     "shell.execute_reply": "2025-02-25T01:38:52.059657Z",
     "shell.execute_reply.started": "2025-02-25T01:38:51.988875Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    num_train_epochs=1,# 총 1 에포크 학습\n",
    "    run_name=\"experiment_2934\",\n",
    "    logging_dir = \"./logs\",\n",
    "    per_device_train_batch_size=4, # 각 디바이스(batch)당 학습 배치 크기\n",
    "    per_device_eval_batch_size=4,# 평가 배치 크기\n",
    "    eval_strategy=\"epoch\", # 에포크마다 평가 수행\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,# 혼합 정밀도 학습 (학습 속도 향상 및 VRAM 최적화)\n",
    "    push_to_hub=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4db3195b-4c8c-479e-9f18-3f44f3ff478d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:38:55.349245Z",
     "iopub.status.busy": "2025-02-25T01:38:55.348549Z",
     "iopub.status.idle": "2025-02-25T01:38:55.358824Z",
     "shell.execute_reply": "2025-02-25T01:38:55.357223Z",
     "shell.execute_reply.started": "2025-02-25T01:38:55.349187Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 토크나이저와 모델에 맞는 데이터 batch 구성\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "777ece90-bfa8-4f05-a7be-9dc88cd86bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:38:56.412696Z",
     "iopub.status.busy": "2025-02-25T01:38:56.411783Z",
     "iopub.status.idle": "2025-02-25T01:38:56.432163Z",
     "shell.execute_reply": "2025-02-25T01:38:56.430767Z",
     "shell.execute_reply.started": "2025-02-25T01:38:56.412635Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    # 소문자화 및 불필요한 공백 제거\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "def compute_em_f1(pred, truth):\n",
    "    pred, truth = normalize_text(pred), normalize_text(truth)\n",
    "    # Exact Match (정확하게 일치하는지)\n",
    "    em = 1 if pred == truth else 0\n",
    "    # F1 계산: 단어 단위 비교\n",
    "    pred_tokens = pred.split()\n",
    "    truth_tokens = truth.split()\n",
    "    common = set(pred_tokens) & set(truth_tokens)\n",
    "    if len(common) == 0:\n",
    "        return em, 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(truth_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return em, f1\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # 토큰을 문자열로 복원 (skip_special_tokens로 불필요한 토큰 제거)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # labels에 pad token(-100)이 포함되어 있을 수 있으므로 복원 시 처리\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    total_em = 0\n",
    "    total_f1 = 0\n",
    "    count = 0\n",
    "    for pred, truth in zip(decoded_preds, decoded_labels):\n",
    "        em, f1 = compute_em_f1(pred, truth)\n",
    "        total_em += em\n",
    "        total_f1 += f1\n",
    "        count += 1\n",
    "    return {\"exact_match\": total_em / count, \"f1\": total_f1 / count}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "448596e5-dfd2-47b1-8600-28e8529a8b8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:47:52.496188Z",
     "iopub.status.busy": "2025-02-25T01:47:52.495483Z",
     "iopub.status.idle": "2025-02-25T01:47:52.511158Z",
     "shell.execute_reply": "2025-02-25T01:47:52.509587Z",
     "shell.execute_reply.started": "2025-02-25T01:47:52.496129Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # 입력에서 labels를 추출 (없으면 input_ids를 labels로 사용)\n",
    "        labels = inputs.get(\"labels\")\n",
    "        if labels is None:\n",
    "            labels = inputs[\"input_ids\"]\n",
    "        \n",
    "        # 모델을 호출하여 outputs를 얻습니다.\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Causal LM의 경우, 일반적으로 logits와 labels를 한 토큰씩 시프트하여 계산합니다.\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        # pad token id는 패딩 토큰을 무시하도록 설정합니다.\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96a95b0b-3bec-41ff-a283-fe9108b958db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:47:53.978046Z",
     "iopub.status.busy": "2025-02-25T01:47:53.977472Z",
     "iopub.status.idle": "2025-02-25T01:47:54.007595Z",
     "shell.execute_reply": "2025-02-25T01:47:54.006185Z",
     "shell.execute_reply.started": "2025-02-25T01:47:53.977998Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20757/2719941267.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# trainer instance creation\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4378b289-9487-4304-a90e-250796335e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:47:55.733404Z",
     "iopub.status.busy": "2025-02-25T01:47:55.732662Z",
     "iopub.status.idle": "2025-02-25T01:47:55.744737Z",
     "shell.execute_reply": "2025-02-25T01:47:55.743211Z",
     "shell.execute_reply.started": "2025-02-25T01:47:55.733343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 전 VRAM 사용량: 4729.25 MB\n"
     ]
    }
   ],
   "source": [
    "# 학습 전 vram 사용량 (only GPU)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vram_before = torch.cuda.memory_allocated()\n",
    "    print(f\"학습 전 VRAM 사용량: {vram_before / (1024**2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e7322d9-4d32-4a33-8a8f-b76feef63b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:35:00.801550Z",
     "iopub.status.busy": "2025-02-25T01:35:00.800835Z",
     "iopub.status.idle": "2025-02-25T01:35:07.295686Z",
     "shell.execute_reply": "2025-02-25T01:35:07.293962Z",
     "shell.execute_reply.started": "2025-02-25T01:35:00.801488Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://repo.ai.gato/registry/repository/pypi-proxy/simple\n",
      "Requirement already satisfied: weave in /home/20215215/.local/lib/python3.10/site-packages (0.51.34)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /home/20215215/.local/lib/python3.10/site-packages (from weave) (5.6.3)\n",
      "Requirement already satisfied: emoji>=2.12.1 in /home/20215215/.local/lib/python3.10/site-packages (from weave) (2.14.1)\n",
      "Requirement already satisfied: gql[aiohttp,requests] in /home/20215215/.local/lib/python3.10/site-packages (from weave) (3.5.1)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from weave) (4.23.0)\n",
      "Requirement already satisfied: numpy>1.21.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from weave) (1.26.4)\n",
      "Requirement already satisfied: packaging>=21.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from weave) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from weave) (2.10.6)\n",
      "Requirement already satisfied: rich in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from weave) (13.9.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from weave) (9.0.0)\n",
      "Requirement already satisfied: uuid-utils>=0.9.0 in /home/20215215/.local/lib/python3.10/site-packages (from weave) (0.10.0)\n",
      "Requirement already satisfied: wandb>=0.17.1 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from weave) (0.19.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from jsonschema>=4.23.0->weave) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from jsonschema>=4.23.0->weave) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from jsonschema>=4.23.0->weave) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from jsonschema>=4.23.0->weave) (0.22.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from pydantic>=2.0.0->weave) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from pydantic>=2.0.0->weave) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from pydantic>=2.0.0->weave) (4.12.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (4.25.6)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.10/site-packages (from wandb>=0.17.1->weave) (75.8.0)\n",
      "Collecting graphql-core<3.2.4,>=3.2 (from gql[aiohttp,requests]->weave)\n",
      "  Using cached http://repo.ai.gato/registry/repository/pypi-proxy/packages/graphql-core/3.2.3/graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from gql[aiohttp,requests]->weave) (1.18.3)\n",
      "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gql[aiohttp,requests]->weave) (2.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from gql[aiohttp,requests]->weave) (3.7.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from gql[aiohttp,requests]->weave) (3.11.12)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/gatoai/python/venv/3.10/lib/python3.10/site-packages (from gql[aiohttp,requests]->weave) (1.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from rich->weave) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from rich->weave) (2.19.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (0.2.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (1.2.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.17.1->weave) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (4.0.12)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->weave) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/gatoai/python/venv/jupyter-3.6.8-py3.10/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (5.0.2)\n",
      "Installing collected packages: graphql-core\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.20.1 requires pyarrow<19,>=4.0.0, but you have pyarrow 19.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed graphql-core-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0625bf0-a0cc-46fc-bbd2-43ef07d615d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:33:01.744325Z",
     "iopub.status.busy": "2025-02-25T01:33:01.743236Z",
     "iopub.status.idle": "2025-02-25T01:33:08.483175Z",
     "shell.execute_reply": "2025-02-25T01:33:08.481252Z",
     "shell.execute_reply.started": "2025-02-25T01:33:01.744249Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhandsomemin\u001b[0m (\u001b[33mhandsomemin-kookmin-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a65278b7-1d6d-4fdb-afba-432053c5fd7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:47:59.004689Z",
     "iopub.status.busy": "2025-02-25T01:47:59.003974Z",
     "iopub.status.idle": "2025-02-25T04:00:23.999761Z",
     "shell.execute_reply": "2025-02-25T04:00:23.993112Z",
     "shell.execute_reply.started": "2025-02-25T01:47:59.004629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15103' max='15102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15102/15102 2:12:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='1444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8/1444 00:01 < 05:23, 4.43 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/20215215/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.81 GiB. GPU 0 has a total capacity of 23.68 GiB of which 3.98 GiB is free. Process 61661 has 28.06 MiB memory in use. Process 46130 has 19.67 GiB memory in use. Of the allocated memory 13.44 GiB is allocated by PyTorch, and 5.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m training_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:2625\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2629\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:3071\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3069\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3071\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3072\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:3025\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3025\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3028\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:4073\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4070\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4072\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4073\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4074\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4083\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:4294\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4292\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   4293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4294\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4296\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:317\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:131\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    134\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    135\u001b[0m     )\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:89\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     86\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     92\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.81 GiB. GPU 0 has a total capacity of 23.68 GiB of which 3.98 GiB is free. Process 61661 has 28.06 MiB memory in use. Process 46130 has 19.67 GiB memory in use. Of the allocated memory 13.44 GiB is allocated by PyTorch, and 5.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_result = trainer.train()\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"학습 시간: {training_time:.2f} 초\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
